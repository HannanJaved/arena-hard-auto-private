# Examples of api config below

# Proprietary models examples
gpt-4-1106-preview:
    model: gpt-4-1106-preview
    endpoints: null
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

gpt-4.1:
    model: gpt-4.1
    endpoints: null
    api_type: openai
    parallel: 64
    max_tokens: 32000
    temperature: 0.0

# gemini-2.5:
#     model: gemini-2.5-pro-preview-03-25
#     endpoints: null
#     api_type: gemini
#     parallel: 50

gemini-2.5:
    model: gemini-2.5-pro-preview-03-25
    endpoints: null
    api_type: vertex
    parallel: 16
    project_id: <project_id>
    regions: <region>

claude-3-7-sonnet-20250219-thinking-16k:
    model: claude-3-7-sonnet-20250219
    endpoints: null
    max_tokens: 20000
    budget_tokens: 16000
    api_type: anthropic_thinking
    parallel: 32

deepseek-r1:
    endpoints:
        - api_key: <your_api_key>
    api_type: deepseek_reasoner
    parallel: 32

claude-3-5-sonnet-20241022:
    model: claude-3-5-sonnet-20241022
    endpoints: null
    max_tokens: 20000
    api_type: anthropic
    parallel: 32

o3-mini-2025-01-31-high:
    model: o3-mini-2025-01-31
    endpoints: null
    reasoning_effort: high
    api_type: openai_thinking
    parallel: 32

gpt-4o-mini-2024-07-18:
    model: gpt-4o-mini
    endpoints: null
    api_type: openai
    parallel: 128
    max_tokens: 8196
    temperature: 0.0

# Local inference examples
qwq-32b:
    model: Qwen/QwQ-32B
    endpoints: null
    api_type: sglang
    local_engine: True
    temperature: 0.6
    end_think_token: "</think>"
    max_tokens: 32000
    
gemma-3-27b-it:
    model: google/gemma-3-27b-it
    endpoints:
        - api_base: http://0.0.0.0:<port_number>/v1
          api_key: '-'
    api_type: openai
    parallel: 128
    max_tokens: 8196
    temperature: 0.0


#******** AWS NOVA MODEL *****************#
aws_nova_light_v1:
    model: aws_nova_light_v1
    model_id: us.amazon.nova-lite-v1:0
    endpoints: null
    api_type: aws_nova
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0

llama3.1-8b:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

llama3.1-8b-instruct:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Meta-Llama-3.1-8B-Instruct
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

neuralmagic-llama3.1-70b-instruct-fp8:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Meta-Llama-3.1-70B-Instruct-FP8
    endpoints:
        - api_base: http://localhost:8001/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** TULU3 MODELS ***************** #

# ******** Rank64 - Alpha 1e-5 - 0.01 WR ***************** #
# Every 6000 steps

tulu3-8b-rank64-alpha1e5-001-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_001/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-001-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_001/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-001-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_001/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-001-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_001/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-001-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_001/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0    

tulu3-8b-rank64-alpha1e5-001-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_001/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-001-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_001/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-001-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_001/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-001-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_001/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank64 - Alpha 1e-5 - 0.05 WR ***************** #
# Every 6000 steps

tulu3-8b-rank64-alpha1e5-005-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_005/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-005-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_005/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-005-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_005/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-005-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_005/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-005-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_005/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-005-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_005/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-005-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_005/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-005-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_005/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-005-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_005/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank64 - Alpha 1e-5 - 0.10 WR ***************** #
# Every 6000 steps

tulu3-8b-rank64-alpha1e5-010-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_010/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-010-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_010/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-010-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_010/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-010-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_010/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-010-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_010/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-010-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_010/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-010-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_010/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-010-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_010/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-010-step50000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_010/step_50000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e5-010-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e5_010/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank64 - Alpha 1e-6 - 0.01 WR ***************** #
# Every 6000 steps

tulu3-8b-rank64-alpha1e6-001-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_001/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-001-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_001/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-001-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_001/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-001-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_001/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-001-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_001/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-001-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_001/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-001-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_001/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-001-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_001/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-001-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_001/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank64 - Alpha 1e-6 - 0.05 WR ***************** #
# Every 6000 steps

tulu3-8b-rank64-alpha1e6-005-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_005/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-005-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_005/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-005-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_005/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-005-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_005/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-005-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_005/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-005-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_005/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-005-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_005/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-005-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_005/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-005-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_005/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank64 - Alpha 1e-6 - 0.10 WR ***************** #
# Every 6000 steps

tulu3-8b-rank64-alpha1e6-010-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_010/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-010-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_010/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-010-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_010/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-010-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_010/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-010-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_010/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-010-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_010/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-010-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_010/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-010-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_010/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-010-step50000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_010/step_50000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha1e6-010-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_1e6_010/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank64 - Alpha 5e-5 - 0.01 WR ***************** #
# Every 6000 steps
tulu3-8b-rank64-alpha5e5-001-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_001/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0


tulu3-8b-rank64-alpha5e5-001-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_001/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-001-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_001/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-001-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_001/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-001-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_001/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-001-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_001/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-001-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_001/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-001-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_001/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-001-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_001/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank64 - Alpha 5e-5 - 0.05 WR ***************** #
# Every 6000 steps

tulu3-8b-rank64-alpha5e5-005-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_005/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-005-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_005/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-005-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_005/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-005-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_005/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-005-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_005/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-005-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_005/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-005-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_005/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-005-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_005/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-005-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_005/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank64 - Alpha 5e-5 - 0.10 WR ***************** #
# Every 6000 steps

tulu3-8b-rank64-alpha5e5-010-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_010/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-010-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_010/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-010-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_010/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-010-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_010/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-010-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_010/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-010-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_010/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-010-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_010/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-010-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_010/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-010-step50000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_010/step_50000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-alpha5e5-010-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/alpha_5e5_010/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank64 - DEFAULT Setting ***************** #
# Every 6000 steps

tulu3-8b-rank64-default-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/Default/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-default-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/Default/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-default-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/Default/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-default-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/Default/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-default-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/Default/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-default-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/Default/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-default-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/Default/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-default-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/Default/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-default-step50000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/Default/step_50000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank64-default-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank64/Default/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank256 - Alpha 1e-5 - 0.01 WR ***************** #
# Every 6000 steps

tulu3-8b-rank256-alpha1e5-001-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_001/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0
tulu3-8b-rank256-alpha1e5-001-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_001/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-001-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_001/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-001-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_001/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-001-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_001/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0    

tulu3-8b-rank256-alpha1e5-001-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_001/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-001-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_001/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-001-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_001/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-001-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_001/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank256 - Alpha 1e-5 - 0.05 WR ***************** #
# Every 6000 steps

tulu3-8b-rank256-alpha1e5-005-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_005/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-005-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_005/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-005-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_005/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-005-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_005/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-005-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_005/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-005-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_005/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-005-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_005/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-005-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_005/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-005-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_005/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank256 - Alpha 1e-5 - 0.10 WR ***************** #
# Every 6000 steps

tulu3-8b-rank256-alpha1e5-010-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_010/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-010-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_010/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-010-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_010/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-010-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_010/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-010-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_010/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-010-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_010/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-010-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_010/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-010-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_010/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-010-step50000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_010/step_50000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e5-010-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e5_010/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank256 - Alpha 1e-6 - 0.01 WR ***************** #
# Every 6000 steps

tulu3-8b-rank256-alpha1e6-001-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_001/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-001-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_001/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-001-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_001/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-001-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_001/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-001-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_001/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-001-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_001/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-001-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_001/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-001-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_001/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-001-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_001/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank256 - Alpha 1e-6 - 0.05 WR ***************** #
# Every 6000 steps

tulu3-8b-rank256-alpha1e6-005-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_005/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-005-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_005/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-005-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_005/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-005-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_005/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-005-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_005/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-005-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_005/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-005-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_005/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-005-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_005/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-005-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_005/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank256 - Alpha 1e-6 - 0.10 WR ***************** #
# Every 6000 steps

tulu3-8b-rank256-alpha1e6-010-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_010/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-010-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_010/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-010-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_010/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-010-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_010/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-010-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_010/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-010-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_010/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-010-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_010/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-010-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_010/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-010-step50000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_010/step_50000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha1e6-010-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_1e6_010/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank256 - Alpha 5e-5 - 0.01 WR ***************** #
# Every 6000 steps
tulu3-8b-rank256-alpha5e5-001-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_001/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-001-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_001/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-001-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_001/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-001-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_001/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-001-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_001/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-001-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_001/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-001-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_001/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-001-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_001/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-001-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_001/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank256 - Alpha 5e-5 - 0.05 WR ***************** #
# Every 6000 steps

tulu3-8b-rank256-alpha5e5-005-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_005/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-005-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_005/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-005-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_005/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-005-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_005/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-005-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_005/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-005-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_005/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-005-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_005/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-005-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_005/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-005-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_005/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank256 - Alpha 5e-5 - 0.10 WR ***************** #
# Every 6000 steps

tulu3-8b-rank256-alpha5e5-010-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_010/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-010-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_010/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-010-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_010/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-010-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_010/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-010-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_010/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-010-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_010/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-010-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_010/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-010-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_010/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-010-step50000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_010/step_50000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-alpha5e5-010-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/alpha_5e5_010/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank256 - DEFAULT Setting ***************** #
# Every 6000 steps

tulu3-8b-rank256-default-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/Default/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-default-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/Default/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-default-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/Default/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-default-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/Default/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-default-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/Default/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-default-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/Default/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-default-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/Default/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-default-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/Default/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-default-step50000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/Default/step_50000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank256-default-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank256/Default/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank1024 - Alpha 1e-5 - 0.01 WR ***************** #
# Every 6000 steps
tulu3-8b-rank1024-alpha1e5-001-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_001/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-001-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_001/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-001-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_001/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-001-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_001/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-001-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_001/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0    

tulu3-8b-rank1024-alpha1e5-001-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_001/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-001-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_001/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-001-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_001/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-001-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_001/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank1024 - Alpha 1e-5 - 0.05 WR ***************** #
# Every 6000 steps

tulu3-8b-rank1024-alpha1e5-005-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_005/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-005-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_005/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-005-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_005/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-005-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_005/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-005-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_005/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-005-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_005/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-005-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_005/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-005-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_005/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-005-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_005/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank1024 - Alpha 1e-5 - 0.10 WR ***************** #
# Every 6000 steps

tulu3-8b-rank1024-alpha1e5-010-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_010/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-010-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_010/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-010-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_010/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-010-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_010/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-010-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_010/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-010-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_010/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-010-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_010/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-010-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_010/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-010-step50000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_010/step_50000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e5-010-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e5_010/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank1024 - Alpha 1e-6 - 0.01 WR ***************** #
# Every 6000 steps

tulu3-8b-rank1024-alpha1e6-001-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_001/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-001-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_001/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-001-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_001/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-001-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_001/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-001-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_001/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-001-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_001/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-001-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_001/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-001-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_001/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-001-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_001/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank1024 - Alpha 1e-6 - 0.05 WR ***************** #
# Every 6000 steps

tulu3-8b-rank1024-alpha1e6-005-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_005/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-005-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_005/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-005-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_005/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-005-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_005/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-005-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_005/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-005-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_005/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-005-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_005/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-005-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_005/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-005-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_005/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank1024 - Alpha 1e-6 - 0.10 WR ***************** #
# Every 6000 steps

tulu3-8b-rank1024-alpha1e6-010-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_010/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-010-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_010/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-010-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_010/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-010-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_010/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-010-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_010/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-010-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_010/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-010-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_010/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-010-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_010/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-010-step50000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_010/step_50000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha1e6-010-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_1e6_010/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank1024 - Alpha 5e-5 - 0.01 WR ***************** #
# Every 6000 steps
tulu3-8b-rank1024-alpha5e5-001-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_001/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0


tulu3-8b-rank1024-alpha5e5-001-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_001/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-001-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_001/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-001-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_001/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-001-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_001/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-001-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_001/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-001-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_001/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-001-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_001/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-001-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_001/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank1024 - Alpha 5e-5 - 0.05 WR ***************** #
# Every 6000 steps

tulu3-8b-rank1024-alpha5e5-005-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_005/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-005-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_005/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-005-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_005/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-005-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_005/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-005-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_005/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-005-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_005/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-005-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_005/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-005-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_005/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-005-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_005/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank1024 - Alpha 5e-5 - 0.10 WR ***************** #
# Every 6000 steps

tulu3-8b-rank1024-alpha5e5-010-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_010/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-010-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_010/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-010-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_010/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-010-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_010/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-010-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_010/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-010-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_010/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-010-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_010/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-010-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_010/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-010-step50000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_010/step_50000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-alpha5e5-010-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/alpha_5e5_010/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

# ******** Rank1024 - DEFAULT Setting ***************** #
# Every 6000 steps

tulu3-8b-rank1024-default-step6000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/Default/step_6000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-default-step12000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/Default/step_12000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-default-step18000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/Default/step_18000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-default-step24000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/Default/step_24000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-default-step30000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/Default/step_30000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-default-step36000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/Default/step_36000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-default-step42000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/Default/step_42000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-default-step48000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/Default/step_48000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-default-step50000:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/Default/step_50000
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

tulu3-8b-rank1024-default-final:
    model: /data/horse/ws/hama901h-BFTranslation/checkpoints/meta-llama/Llama-3.1-8B/tulu3/w_checkpoints/Rank1024/Default/final
    endpoints:
        - api_base: http://localhost:8000/v1
          api_key: '-'
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0
