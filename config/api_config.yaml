# Examples of api config below

# Proprietary models examples
gpt-4-1106-preview:
    model: gpt-4-1106-preview
    endpoints: null
    api_type: openai
    parallel: 32
    max_tokens: 4096
    temperature: 0.0

gpt-4.1:
    model: gpt-4.1
    endpoints: null
    api_type: openai
    parallel: 64
    max_tokens: 32000
    temperature: 0.0

# gemini-2.5:
#     model: gemini-2.5-pro-preview-03-25
#     endpoints: null
#     api_type: gemini
#     parallel: 50

gemini-2.5:
    model: gemini-2.5-pro-preview-03-25
    endpoints: null
    api_type: vertex
    parallel: 16
    project_id: <project_id>
    regions: <region>

claude-3-7-sonnet-20250219-thinking-16k:
    model: claude-3-7-sonnet-20250219
    endpoints: null
    max_tokens: 20000
    budget_tokens: 16000
    api_type: anthropic_thinking
    parallel: 32

deepseek-r1:
    endpoints:
        - api_key: <your_api_key>
    api_type: deepseek_reasoner
    parallel: 32

claude-3-5-sonnet-20241022:
    model: claude-3-5-sonnet-20241022
    endpoints: null
    max_tokens: 20000
    api_type: anthropic
    parallel: 32

o3-mini-2025-01-31-high:
    model: o3-mini-2025-01-31
    endpoints: null
    reasoning_effort: high
    api_type: openai_thinking
    parallel: 32

gpt-4o-mini-2024-07-18:
    model: gpt-4o-mini
    endpoints: null
    api_type: openai
    parallel: 128
    max_tokens: 8196
    temperature: 0.0

# Local inference examples
qwq-32b:
    model: Qwen/QwQ-32B
    endpoints: null
    api_type: sglang
    local_engine: True
    temperature: 0.6
    end_think_token: "</think>"
    max_tokens: 32000
    
gemma-3-27b-it:
    model: google/gemma-3-27b-it
    endpoints:
        - api_base: http://0.0.0.0:<port_number>/v1
          api_key: '-'
    api_type: openai
    parallel: 128
    max_tokens: 8196
    temperature: 0.0

#********AWS CLAUDE MODELS *****************#
aws_claude3_haiku:
    model: aws_claude3_haiku
    model_id: anthropic.claude-3-haiku-20240307-v1:0
    endpoints: null
    api_type: aws_claude
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_claude3_sonnet:
    model: aws_claude3_sonnet
    model_id: anthropic.claude-3-sonnet-20240229-v1:0
    endpoints: null
    api_type: aws_claude
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_claude35_sonnet:
    model: aws_claude35_sonnet
    model_id: anthropic.claude-3-5-sonnet-20240620-v1:0
    endpoints: null
    api_type: aws_claude
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
aws_claude3_opus:
    model: aws_claude3_opus
    model_id: anthropic.claude-3-opus-20240229-v1:0
    endpoints: null
    api_type: aws_claude
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0

aws_claude35_sonnet_v2:
    model: aws_claude35_sonnet_v2
    model_id: anthropic.claude-3-5-sonnet-20241022-v2:0
    endpoints: null
    api_type: aws_claude
    parallel: 8
    max_tokens: 4096
    temperature: 0.0

aws_claude37_sonnet:
    model: aws_claude37_sonnet
    model_id: us.anthropic.claude-3-7-sonnet-20250219-v1:0 
    endpoints: null
    api_type: aws_claude
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
#********AWS MISTRAL MODELS *****************#    
aws_mistral_7b_instruct:
    model: aws_mistral_7b_instruct
    model_id: mistral.mistral-7b-instruct-v0:2
    endpoints: null
    api_type: aws_mistral
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_mistral_8x7b_instruct:
    model: aws_mistral_8x7b_instruct
    model_id: mistral.mixtral-8x7b-instruct-v0:1 
    endpoints: null
    api_type: aws_mistral
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_mistral_large_v1:
    model: aws_mistral_large_v1
    model_id: mistral.mistral-large-2402-v1:0
    endpoints: null
    api_type: aws_mistral
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
aws_mistral_large_v2:
    model: aws_mistral_large_v2
    model_id: mistral.mistral-large-2407-v1:0
    endpoints: null
    api_type: aws_mistral
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_mistral_small:
    model: aws_mistral_small
    model_id: mistral.mistral-small-2402-v1:0
    endpoints: null
    api_type: aws_mistral
    parallel: 8
    max_tokens: 4096
    temperature: 0.0

aws_pixtral_large:
    model: aws_pixtral_large
    model_id: us.mistral.pixtral-large-2502-v1:0
    endpoints: null
    api_type: aws_mistral
    parallel: 8
    max_tokens: 4096
    temperature: 0.0


#********AWS LLAMA MODELS *****************#
aws_llama_3_8b_instruct:
    model: aws_llama_3_8b_instruct
    model_id: meta.llama3-8b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8
    max_tokens: 4096
    temperature: 0.0

aws_llama_3_70b_instruct:
    model: aws_llama_3_70b_instruct
    model_id: meta.llama3-70b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_3_1_8b_instruct:
    model: aws_llama_3_1_8b_instruct
    model_id: meta.llama3-1-8b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_3_1_70b_instruct:
    model: aws_llama_3_1_70b_instruct
    model_id: meta.llama3-1-70b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_3_1_405b_instruct:
    model: aws_llama_3_1_405b_instruct
    model_id: meta.llama3-1-405b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0

aws_llama_3_2_1b_instruct:
    model: aws_llama_3_2_1b_instruct
    model_id: meta.llama3-2-1b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
    
aws_llama_3_2_3b_instruct:
    model: aws_llama_3_2_3b_instruct
    model_id: meta.llama3-2-3b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_3_2_11b_instruct:
    model: aws_llama_3_2_11b_instruct
    model_id: meta.llama3-2-11b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_3_2_90b_instruct:
    model: aws_llama_3_2_90b_instruct
    model_id: meta.llama3-2-90b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8  
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_2_chat_13b:
    model: aws_llama_2_chat_13b
    model_id: meta.llama2-13b-chat-v1
    endpoints: null
    api_type: aws_llama
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_2_chat_70b:
    model: aws_llama_2_chat_70b
    model_id: meta.llama2-70b-chat-v1
    endpoints: null
    api_type: aws_llama
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0

#******** AWS NOVA MODELS *****************#
aws_nova_light_v1:
    model: aws_nova_light_v1
    model_id: us.amazon.nova-lite-v1:0
    endpoints: null
    api_type: aws_nova
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
aws_nova_pro_v1:
    model: aws_nova_pro_v1
    model_id: us.amazon.nova-pro-v1:0
    endpoints: null
    api_type: aws_nova
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
aws_nova_micro_v1:
    model: aws_nova_micro_v1
    model_id: us.amazon.nova-micro-v1:0
    endpoints: null
    api_type: aws_nova
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0

aws_nova_premier_v1: 
    model: aws_nova_premier_v1
    model_id: us.amazon.nova-premier-v1:0
    endpoints: null
    api_type: aws_nova
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
#******** AWS DEEPSEEK MODELS *****************#
aws_deepseek-r1_v1:
    model: aws_deepseek-r1_v1
    model_id: us.deepseek.r1-v1:0
    endpoints: null
    api_type: aws_deepseek
    parallel: 8   
    max_tokens: 4096
    temperature: 0.0
